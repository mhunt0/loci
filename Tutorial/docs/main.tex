\documentclass[10pt,epsf]{book}
\title { Loci : A Tutorial }

\setlength{\textheight}{7.75in}
\setlength{\textwidth}{5.7in}
\setlength{\parskip}{3mm}
\setlength{\parindent}{0.0in}
\setlength{\topmargin}{5mm}
%\setlength{\bottommargin}{5mm}
\setlength{\headheight}{5mm}
\setlength{\headsep}{5mm}
\setlength{\oddsidemargin}{2cm}
\setlength{\evensidemargin}{2cm}
\unitlength=1in

\usepackage{epsf}    % used for importing encapsulated postscript figures
\usepackage{amsmath} % used for extended formula formatting tools
\usepackage{amssymb}
\usepackage{theorem}
\usepackage{euscript}
\usepackage{longtable}

\begin{document}
%\small
\tableofcontents
%\listoffigures
\maketitle
\thispagestyle{empty}
\newpage
\setcounter{page}{1}

\include{intro}

\chapter{ Basic Concepts }

\section{Notation used in this document}
In this document we use the {\tt typewriter} font to distinguish
actual Loci programming keywords, classes, and data-structures.

\section{ Compiling Loci Programs }

The most direct way to compile Loci programs is to use the Makefile
template provided in this tutorial.  It is usually as simple as
including the {\tt Loci.conf} file that comes as part of your Loci
installation.  See appendix \ref{chap:makefile} for an example
makefile or refer to example Makefiles in the tutorial directory.


\section{Loci Initialization}

Before any of the main Loci functionality can be used (that is the
components that follow this section), Loci must be initialized.  Loci
has an initialize function that must be called before executing Loci
functionality and a finalize method that must be called just before
exiting the program.  Note, that the include file {\tt \#include
  <Loci.h>} includes all commonly used components of the Loci
framework, including definitions of the initialization routines.  For
example, see below:
\clearpage
\begin{verbatim}
#include <Loci.h>

int main(int argc, char *argv[]) {
   // Initialize Loci
   Loci::Init(&argc, &argv) ;

   // ...
   // Loci Program
   // ...

   // Before exiting, call finalize to let Loci clean up.
   Loci::Finalize() ;
   return 0 ;
}
\end{verbatim}

\section{Entities, Sets, and Sequences}

Probably the most fundamental concept of Loci is that of entities.  In
Loci, computations are represented by associating values with
entities.  Although entities can be considered in rather abstract
terms, in Loci we often will often interchange the meaning of entity
with the integer identifier that is used to label a given entity.
Thus we may talk of entity $1$ when we are really referring to the
entity labeled $1$.  Note, that while the entity itself is immutable,
its label may change in the course of executing a Loci program, and in
particular when Loci schedules parallel programs.  Generally the user
is unaware of this fact, but it can become important in a few cases
that will be mentioned in later examples.

As important as the concept of entity is the concept of entity
collections.  Generally, it is useful to consider groups of entities
that have similar attributes.  In Loci we have two provided types for
representing sets of entities: 1) the {\tt interval} and 2) the {\tt
  entitySet}.  For example, if we wish to represent the entities
labeled from $1$ to $100$ we would use the Loci type {\tt
  interval(1,100)}.  On the other hand, the {\tt entitySet} can be
used to represent arbitrary collections of entities.  Once we have
described a collection of entities using the {\tt entitySet} class we
can also create new sets of entities using unions, intersections, and
other useful set operations.

The {\tt entitySet} class provides true set semantics.
That is, ordering of insertion is not preserved and there is no
duplication.  Either an entity is in the set or it is not.  If we need
to preserve the order of entities for looping or other control then we
use the {\tt sequence } class.  The {\tt sequence} class provides
operations for concatenation and reversal and can be thought
of as a list of entity labels.  It should be noted that users
generally don't create sequences in Loci, but rather the scheduler
generates sequence of entities for computations.  However, if there is
ever a need to keep track of a particular ordering of entities, then
sequences are the data-structure that accomplishes this task.

The following program segment (included with the tutorial programs)
provides examples of how to create and use sets and sequence of
entities in Loci.

\include{entities_cc}

\section{Loci Containers}

In Loci, containers are entity based.  That is, a container provides
an association between entities and values.  There are two basic types
of containers: stores and parameters.  Stores are used to associate
values with entities, while parameters are used to associate a value
with a sent of entities.  For example, in a simulation the each nodes
of a mesh will have a position vector, and this will be represented in
Loci using a store container.  However, the time-step of a simulation
is a single value that is shared by all of the simulation entities and
this will be represented using a parameter in Loci.  The {\tt store} is a
templated container that can be used to contain arbitrary types.  For
example:
\begin{verbatim}
  // We create a store of floats
  store<float> x ;
  // We create a store of float vectors, this is OK also
  store<std::vector<float> > particles ;
\end{verbatim}

Once we create the store container we can use the {\tt allocate()}
method to allocate values over some set of entities.  For example, to
allocate the above containers over 100 entities we would use code such
as:
\begin{verbatim}
   // allocate stores x and particles
   entitySet alloc_set = interval(1,100) ;
   x.allocate(alloc_set) ;
   particles.allocate(alloc_set) ;
\end{verbatim}

After allocating the container, we can access the values of the
container using the array operator.  In this sense a store looks like
an array with array bounds that are general sets.  For example, if we
want to initialize the contents of our containers we might use code
that iterates over the allocated set such as the following:
\begin{verbatim}
  // initialize the container to the value zero
  entitySet::const_iterator ei ;
  for( ei = alloc_set.begin(); ei != alloc_set.end(); ++ei) {
    x[*ei] = 0 ;
    particles[*ei].push_back(0) ; // Calling vector method push_back()
  }
\end{verbatim}

Note, we can also query the domain, or defining set of entities, for any container by using the {\tt domain()} method.  For example
\begin{verbatim}
  // write out all of store x to cout
  entitySet xdom = x.domain() ;
  for( ei = xdom.begin(); ei != xdom.end(); ++ei) 
    cout << "x["<<*ei<<"]=" << x[*ei] << endl ;
\end{verbatim}

For parameters Loci provides the {\tt param} templated class.  This
class can also be used to hold arbitrary types.  It associates a given
value with a collection of entities.  By default this collection of
entities is the universal set, but there are methods for limiting this
to any subset of entities.  We create a param similar to the store
type with code such as:
\begin{verbatim}
  // Create the timestep
  param<float> timestep ;
\end{verbatim}
We then can assign a value to the parameter using the dereferencing
{\tt *} operator.  For example:
\begin{verbatim}
   // Set the timestep to 1ms
   *timestep = 1e-3 ;
\end{verbatim}
Similarly we can assign a value for a subset of entities (such as a
boundary entities) using the parameters facility as well.  By default
the param associates a value with all possible entities, but this
association can be changed using the {\tt set\_entitySet()} member
function.  For example:
\begin{verbatim}
  param<real> Twall ; // Create wall temperature
  Twall = 300 ;
  // Constraint Twall to only apply to boundary entities (as given)
  entitySet wallBoundary = interval(1000,1500) ; 
  Twall.set_entitySet(wallBoundary) ;
\end{verbatim}

%storeVec and storeMat

% Parameters provide a way of associating a single value with a set of
% entities.  With respect to the set of entities that they are associated
% with, parameter variables behave much like global variables.  There are
% two parameter containers: param and blackbox, which differ in their
% scope and intended usage.  The param variables are synchronized over all
% of the processors and can only be used with first class objects that Loci
% knows about.  The blackbox variables are not synchronized over processors
% and can hold any data type.  Blackbox containers are intended to be used
% to hold data structures for third party libraries, or other data types
% that Loci is not able to manage directly.

% Stores provide a one-to-one correspondence between entities and values.
% In shorter terms, stores look like very flexible arrays.  The stores
% come in a variety of forms that allow various types of run-time selection
% of the sizes of the types they contain.  For example, the storeVec
% provides a store that contains vectors whose size isn't specified until
% run time.


\section{Loci Relations}

In addition to containers, Loci provides ways of describing
relationships between entities.  The simplest of these relationships
is the constraint.  The constraint simply identifies a grouping of
entities and is used to assign attributes to entities.  For example, a
boundary condition may be specified by placing those entities in the
boundary in a boundary constraint.  For example, suppose entities
labeled $1,2,$ and $3$ are at an inflow boundary, we might construct
such a structure by creating a constraint and assigning these entities
to the constraint.  For example:
\begin{verbatim}
   // set inflow constraint
   constraint inflow ;
   *inflow = entitySet(interval(1,3)) ;
\end{verbatim}
Alternatively, constraints might be used to enable and disable some
feature in the solver.  In this setting a constraint may either
contain the empty set, or may contain all possible entities.  For
example, we might use a constraint to select between having viscous
terms or not using the following type of setup:
\begin{verbatim}

   constraint viscous ;
   *viscous = EMPTY ; // default to not empty 
   if(mu_set) // If viscosity set, then enable viscous terms
     *viscous = ~EMPTY ; // Constraint set to contain all entities
\end{verbatim}
Note that in this setup we use the tilde to complement the {\tt EMPTY} set
to achieve the result of identifying every entity.  This set will contain
all entities not in the empty set (e.g. everything).  

Constraints can identify a group of entities are related, however it
cannot provide a relationship from one entity to another (e.g. how do
faces relate to cells, what nodes make up a face, etc).  We use a {\tt
  Map} container to describe these types of relationships.  The {\tt
  Map} is used to relate any given entity with another.  The map is
analogous to a {\tt store} that contains entities, and as such can be
allocated, and assigned values similar to a generalized array much
like the {\tt store} container.  For example, if we wanted to create a {\tt Map} for all entities to the left of an entity in a number line we might write code such as:
\begin{verbatim}
  entitySet nodes = interval(0,10) ; // A number line from 0 to 10
  entitySet left = (nodes >> 1) & nodes ; // Shift set to get nodes that
                                          // have a left side
  // Create Maping from current node to the node to the left
  Map leftNode ;
  leftNode.allocate(left) ; // Allocate over all entities tha have a left side
  // Assign left node by looping over left nodes and nodes at the same time
  entitySet::const_iterator ni = nodes.begin() ;
  for(entitySet::const_iterator li=left.begin();li!=left.begin();++li,++ni)
    leftNode[*li] = *ni ; // node *ni is to the left of node *li
\end{verbatim}

Loci also includes other types of map containers such as {\tt mapVec}
and {\tt multiMap} which provide mechanisms for having multiple entity
associations.  They will be discussed later in the tutorial as we get
to more advanced topics.

\section{Databases within Loci}

One of the facilities that Loci provides is the management of fact and
rule databases.  The fact database provides a repository for the
containers described earlier.  Once a container is put in the fact
database, it can be retrieved at a later time by its assigned name.  For example, if we wanted to store the {\tt leftNode } map computed in the previous section into a Loci fact database we would use the {\tt create\_fact()} method.  For example:
\begin{verbatim}
  fact_db facts ; // Create the fact database
  
  // Insert leftNode into the fact database
  facts.create_fact("leftNode",leftNode) ;
\end{verbatim}

The fact can later be retrieved from the fact database using the {\tt get\_fact()} method.  For example, we can pass the fact database into a function and then use it to get a copy of the {\tt leftNode} map with code such as:
\begin{verbatim}
void worker(fact_db &facts) {
  Map leftNode ;  // Create Map container
  leftNode = facts.get_fact("leftNode") ;
  // ... code using leftNode follows
}
\end{verbatim}

In addition to the fact database, Loci provides a facility for storing
rules that describe computations that can generate new facts.  We will
describe rules more in the following section.  Usually the user has
minimal interaction with the rule database.  In general, the user
creates the rule database and fills it with rules that were registered
with the system automatically before {\tt main()} executes.  As in the
following example:
\begin{verbatim}
  rule_db rdb ; // create the rule database called rdb
  rdb.add_rules(global_rule_list) ; // Add all registered rules to the database
\end{verbatim}
Both the fact database and the rule database are fundamental to
programming using the Loci framework.  The fact database describes
what you know about a problem, the rule database describes what you
can derive.  Both of these components are used to make Loci queries
using the {\tt Loci::makeQuery} command.  For example to query for the
computed temperature one would implement code such as:
\begin{verbatim}
 // Query Loci for fact derived fact 'temperature'
  if(!Loci::makeQuery(rdb,facts,"temperature")) {
    cerr << "query failed!" << endl ;
  }
\end{verbatim}
In most Loci programs, the user queries for the generic variable
``solution'' which just indicates that final solution to the problem.
For most time dependent problems, the most interesting information is
the intermediate values obtained in the time evolution, not the final
value.

\section{A Simple Example}

Since specification languages are not typically encountered when
implementing discretization based numerical schemes, a simple example
problem will be used to illustrate the basic ideas and motivations
behind the rule-based approach to application development.  For this
purpose the one-dimensional linear diffusion problem is selected.  A formal
description of this problem in the interval  $x\in[0,1]$ for a given
diffusion constant $\nu$, is described by the equations
\begin{align}
\label{eq3:diffuse}
u_t      & =  \nu u_{xx},~ x \in (0,1), t>0,\\
\label{eq3:diffuseinitial}
u(x,0)   & =  f(x),~ x \in [0,1],\\
\label{eq3:diffuseb0}
u_x(0,t) & =  g(t), \mbox{ where } g(0) = f_x(0), \mbox{ and }\\
\label{eq3:diffuseb1}
u_x(1,t) & =  h(t), \mbox{ where } h(0) = f_x(1).
\end{align}

Equations (\ref{eq3:diffuse}) through (\ref{eq3:diffuseb1}) formally
define the problem to be solved; however, the methodology of solution
is left open.  A complete specification for finding an analytical
solution might be stated as follows: using the Laplace transforms and
associated algebraic identities, find the value of the function
$u(x,t)$ such that the definitions given in equations
(\ref{eq3:diffuse}) through (\ref{eq3:diffuseb1}) are satisfied.
Notice that this specification contains three distinct parts: 1) a
definition of the problem, 2) a collection of transformations, and 3)
a goal that must be satisfied.  For this case, an analytic solution to
the problem may be found for a few specific
functions $f(x)$, $g(t)$, and $h(t)$.  In general, however, analytical
solutions to PDE problems of interest to engineering are either
impractical or impossible, due to the complexity of the geometries
involved and the non-linearity of the equations themselves.  For this
reason, approximate numerical methods are often used to solve PDE
based problems.  However, the basic approach of problem and solution
specification through definitions, transformations, and goals 
applies equally well to numerical solution methods.  The question is,
how does one formally specify the problem and solution methodology for
these numerical methods in this definition-transformation-goal style?


\subsection{A Finite Volume Solution}

The first step in numerically approximating the function $u(x,t)$ is
the discretization of the spatial domain (in this case the interval
$[0,1]$).  For this example, the finite volume discretization method
is chosen.\footnote{Other discretization schemes follow similar lines.
  See Appendix \ref{app:numerical} for alternative discretization
  examples.} Using this discretization approach, the interval $[0,1]$
is divided into $N$ sub-intervals, as illustrated in figure
\ref{fig3:oned}.  To facilitate describing the discretization process,
the $N$ sub-intervals, or cells, are labeled by $c = N+1, \cdots, 2N$,
while the interfaces at the boundaries of sub-intervals are labeled $i
= 0, \cdots, N$.  Note that the typical labeling used for theoretical
purposes would include half step labels for the interfaces, while a
typical unstructured application code might label both cells and
interfaces starting from zero and use context to distinguish between
the two cases.  However, for the purposes of automating reasoning
about these entities of computations it is assumed that these labels
are integers and that independent computational sites (in this case,
cells and interfaces) are labeled distinctly.  The proposed labeling
satisfies both of these constraints.

\begin{figure}[htbp]
 \centerline{
  \epsfxsize=5.50in
  \epsfbox{figures/one-d.eps}}
 \caption{A Discretization of the Interval $[0,1]$}
 \label{fig3:oned}
\end{figure}

As illustrated in figure \ref{fig3:oned}, the discretization yields
$N+1$ interfaces which have the positions given by
\begin{equation}
x = \lbrace (i,x_i) | i \in [0, \cdots, N], x_i = i/N \rbrace.
\label{eq3:interfacex}
\end{equation}
Notice that the variable $x$ in this equation is described by a set of
ordered pairs where the first entry is the entity identifier, whereas
the second entry is the value bound to that entity.  This is a more
general abstraction of the array.  For example {\tt x[i]} is
represented abstractly as $\lbrace x_i | (i,x_i) \in x \rbrace$.

In addition, this discretization yields $N$ intervals, or cells, which
are represented by the mappings between cells and interfaces by way of
the following relationships
\begin{equation}
\begin{array}{rcl}
il & = & \lbrace (c,l) | c \in [N+1, \cdots, 2N], l = c-N-1 \rbrace,\\
ir & = & \lbrace (c,r) | c \in [N+1, \cdots, 2N], r = c-N \rbrace.\\
\end{array}
\label{eq3:cellmaps}
\end{equation}
The mappings $il$ and $ir$ provide mappings from every cell to their
left and right interfaces.  The domain of $ir$ and $il$ is $[N+1,
\cdots, 2N]$, or the cells in the discretization, while the ranges are
$\mathrm{ran}(ir) = [0, \cdots, N-1]$ and $\mathrm{ran}(il) = [1, \cdots, N]$.  This
mapping is used to conveniently describe subscripts, {\it i.e.}
$x_{c-N} = ir \rightarrow x$, where the composition operator,
$\rightarrow$, defines the application of the mapping, as in
\begin{equation}
il\rightarrow x = \lbrace (c,x_l) | (c,l) \in il, (l,x_l) \in x \rbrace.
\end{equation}
Using this notation, it is possible to conveniently describe cell
based calculations.  For example, a generic description of each cell
center is given by
\begin{equation}
\label{eq3:cellcenter}
x = (ir \rightarrow x + il \rightarrow x)/2.
\end{equation}
Note that the definition of $x$ provided by equation
(\ref{eq3:cellcenter}) is only applicable to cells since only cells are in
the domain of maps $ir$ and $il$; however, this does not prevent the
definition of $x$ for other entities (for example, interfaces) via
other rules.

The mappings $il$ and $ir$ are used to describe the first
step of the finite volume discretization, where integration of
equation (\ref{eq3:diffuse}) over each cell produces the equation
\begin{equation}
\int_{t_n}^{t_{n+1}} \int_{il \rightarrow x}^{ir \rightarrow x} u_t dx
dt = \int_{t_n}^{t_{n+1}} \nu(ir \rightarrow u_x - il \rightarrow
u_x) dt.
\label{eq3:diffinteg}
\end{equation}

Equation (\ref{eq3:diffinteg}) is an exact equation, which can be
integrated numerically to obtain a numerical solution algorithm.
For example, a first order end-point rule is applied to the time
integrations while a second order mid-point rule is applied to the space
integrations to obtain a finite-volume numerical method, expressed as
\begin{equation}
u^{n+1} = u^n + \nu \Delta t
\left[ \frac{ir \rightarrow u^n_x - il \rightarrow u^n_x}
          {ir\rightarrow x - il\rightarrow x}\right]
\label{eq3:timeadvance}
\end{equation}

Equation (\ref{eq3:timeadvance}) describes the numerical method for
advancing the time step, but it is not complete.  The gradient term,
$u^n_x$, located at the interfaces has not been defined as a numerical
approximation.  The most straightforward approximation for $u_x$ is a
central difference formula using the values at the cell centers at
either side of the interface.  In order to perform this calculation it
will be convenient to have a mapping from interfaces to cells similar to
the development of $il$ and $ir$.  These mappings are defined by
the relations
\begin{equation}
\begin{array}{rcl}
cl & = & \lbrace (i,l) | i \in [1, \cdots, N], l = i+N \rbrace,\\
cr & = & \lbrace (i,r) | i \in [0, \cdots, N-1], r = i+N+1 \rbrace.\\
\end{array}
\label{eq3:facemaps}
\end{equation}

Using the definitions of $cl$ and $cr$ of (\ref{eq3:facemaps}), a
numerical approximation to the gradient can be given as
\begin{equation}
u_x^n = \frac{cr\rightarrow u^n - cl\rightarrow u^n}
           {cr\rightarrow x - cl\rightarrow x}.
\label{eq3:ux}
\end{equation}
Notice that this equation uses the x-coordinate at the cell centers
that is computed by equation (\ref{eq3:cellcenter}).  In addition,
since this rule uses both maps $cr$ and $cl$, it only defines $u_x$ on
the intersection of the domains of $cr$ and $cl$, given by $[1, \cdots,
N-1]$.  By this reasoning, equation (\ref{eq3:ux}) only provides
gradients at the internal faces of the domain.  The gradient at the
boundary faces is provided by the boundary conditions given in
equations (\ref{eq3:diffuseb0}) and (\ref{eq3:diffuseb1}).  The
question is, how do these boundary conditions specify $u_x$ at the
boundaries without specifying $u_x$ everywhere in the domain?
Obviously additional information must be provided that constrains
the application of boundary condition gradients only to the boundary
interfaces.  A solution to this problem can be found with the
observation that the boundary interfaces have the distinction that
either $cl$ is defined or $cr$ is defined, but not both.  Using this
fact, the rules for calculating the boundary gradients can be given by
\begin{equation}
\begin{aligned}
u_x^n & = g(n \Delta t), \mbox{constraint}\lbrace \neg \mathrm{dom}(cl) \wedge
\mathrm{dom}(cr) \rbrace, \label{eq3:brule0}\\
u_x^n & = h(n \Delta t), \mbox{constraint}\lbrace \mathrm{dom}(cl)
\wedge \neg \mathrm{dom}(cr) \rbrace. %\label{eq3:brule1}
\end{aligned}
\end{equation}
Here the constraint term added to the rule indicates a constraint on
the application of the rule.  In this case it constrains the
application of the boundary conditions to the appropriate boundary faces.

At this point, the computation of $u^{n+1}$ from $u^n$ is completely
specified.  However, before any such iteration can begin, an initial
value, or $u^{n=0}$, must be given.  To be consistent with the finite
volume formulation, the derivation of the initial conditions begins
with the integral form of equation (\ref{eq3:diffuseinitial}), given by
\begin{equation}
\int^{ir\rightarrow x}_{il\rightarrow x} u^{n=0} dx =
\int^{ir\rightarrow x}_{il\rightarrow x} f(x) dx.
\end{equation}
Using a midpoint rule to numerically integrate this equation one
obtains the rule
\begin{equation}
u^{n=0} = f(x), \mbox{constraint}\lbrace (il,ir)\rightarrow x\rbrace.
\label{eq3:ic}
\end{equation}
For this rule, the constraint is used to indicate that although the
coordinates of the interfaces cancel in the derivation, their
existence is predicated by the integration.  In other words, the
derivation assumed a cell perspective that includes left and right
interface positions.

\subsection{On Problem Specification}

For an analytic solution method, equations (\ref{eq3:diffuse}) through
(\ref{eq3:diffuseb1}) are sufficient to define the problem at hand.
For numerical solution methods, additional definitions are required,
due to the fact that these are inexact methods.  For example, there
are often tradeoffs between discretization and accuracy that require
additional specification.  In addition, since discretization for
complex geometries (grid generation) is not a completely automatic
process, the discretization becomes part of the problem definition for
numerical solution methods.  For the example diffusion problem already
introduced, the definition of the numerical problem consists of
spatially independent information such as the diffusion constant
$\nu$, the initial condition function $f(x)$, the numerical time step
$\Delta t$, and a representation of the discretization of space.  The
discretization of space is given by a set of positions,
(\ref{eq3:interfacex}), and the collection of mappings given in
(\ref{eq3:cellmaps}) and (\ref{eq3:facemaps}).  Table
\ref{table3:facts} summarizes these formal definitions for the example
diffusion problem.


\begin{table}[htbp]
\caption{ A Summary of Definitions for the Example Diffusion
  Problem}
\label{table3:facts}
\begin{center}
  \begin{tabular}{|l|l|}
    \hline
    fact      & meaning \\
    \hline
    $\nu$     & given diffusion constant  \\
    $f(x)$     & given initial condition  \\
    $g(t)$     & given left bc \\
    $h(t)$     & given right bc  \\
    $\Delta t$& given time-step  \\
    $x$       & $\lbrace (i,x_i) | i \in [0, \cdots, N], x_i = i/N    \rbrace$\\
    $il$      & $\lbrace (c,l)   | c \in [N+1, \cdots, 2N], l = c-N-1 \rbrace$\\
    $ir$      & $\lbrace (c,r)   | c \in [N+1, \cdots, 2N], r = c-N   \rbrace$\\
    $cl$      & $\lbrace (i,l)   | i \in [1, \cdots, N], l = i+N      \rbrace$\\
    $cr$      & $\lbrace (i,r)   | i \in [0, \cdots, N-1], r = i+N+1  \rbrace$\\
    \hline
  \end{tabular}
\end{center}
\end{table}



\subsection{On Specification of Process}

Given the definition of the problem, the process of solving the
problem is dictated by a prescribed set of transformations.  For
example, consider equation (\ref{eq3:cellcenter}) as an example of a
transformation that transforms $x$ located at $il$ and $ir$ into a
cell $x$.  To simplify discussions of the structure of the
calculations, the transformation rules are represented by a rule
signature that is denoted by a list of targets of the transformation
delineated from the sources of the transformation by the left arrow
symbol, '$\leftarrow$'.  Thus the cell center position calculation is
represented by the rule signature $x \leftarrow (ir,il)\rightarrow x$.
This rule signature represents the augmentation of the set of ordered
pairs defined in equation (\ref{eq3:interfacex}) with the additional
set given as
\begin{equation}
x\leftarrow\lbrace (c, x_c) |  x_c = (x_l + x_r)/2,
                               (l,x_l) \in x, (r,x_r) \in x, 
                               (c,l) \in il, (c,r) \in ir \rbrace.
\end{equation}
For the moment, the augmentation of $x$ with this set can be
considered as a set union operation, with the caveat that it will
become more complex once issues of specification consistency are
considered.  Given this notation, the specification of the finite
volume scheme derived in this section can be summarized by six rules
given in table \ref{table3:rules}.

\begin{table}[htbp]
\caption{ A Summary of Rules Describing the Solution of the Example
    Diffusion Problem.}
\label{table3:rules}
\begin{center}
  \begin{tabular}{|l|l|l|}
    \hline
    Rule  & Rule Signature & Equation\\
    \hline
    Rule 1 & $x \leftarrow (ir,il)\rightarrow x $ &
    (\ref{eq3:cellcenter})\\
    Rule 2 & $u^{n+1} \leftarrow u^n,(ir,il)\rightarrow(u_x^n,x)$ &
    (\ref{eq3:timeadvance})\\
    Rule 3 & $u_x^n \leftarrow (cr,cl)\rightarrow(u^n,x)$ &
    (\ref{eq3:ux})\\
    Rule 4 & $u_x^n \leftarrow  g, n, \Delta t, \mbox{constraint}\lbrace 
    \neg \mathrm{dom}(cl) \wedge \mathrm{dom}(cr) \rbrace$&
    (\ref{eq3:brule0})\\
    Rule 5 & $u_x^n \leftarrow  h, n, \Delta t, \mbox{constraint}\lbrace 
    \mathrm{dom}(cl) \wedge \neg \mathrm{dom}(cr) \rbrace$ &
    (\ref{eq3:brule1})\\
    Rule 6 & $u^{n=0} \leftarrow f,x,\mbox{constraint}\lbrace(il,ir)\rightarrow
    x\rbrace $ &
    (\ref{eq3:ic})\\
    \hline
  \end{tabular}
\end{center}
\end{table}

\subsection{Implementing the  Problem Specification}

How does one translate the definition of the problem given in table
\ref{table3:facts} and the specification of the solution method given
in table \ref{table3:rules} into an implementation that can solve for
$u^n, n=0,1, \cdots$?  One accomplishes this implementation by
starting from what is known and using the rules of table
\ref{table3:rules} to incrementally derive the specified goal.  As in
Prolog\cite{Clocksin.87}\cite{Sterling.86}, rule resolution, a
generalization of modus ponens, is used to produce these incremental
derivations.  In other words, rules are applied where their sources
are satisfied.  The set of entities that satisfy a rule's sources is
the context of the rule.  For example, Rule 1 from table
\ref{table3:rules} can be resolved with the definitions of $il$, $ir$,
and $x$ given in table \ref{table3:facts} for the entities numbered
$N+1, \cdots, 2N$.  Similarly, once Rule 1 is resolved, Rule 6 can be
resolved using the values of $x$ generated by Rule 1.  Iteration is
recovered by way of induction.  For example, if a rule generates
$u^{n=0}$ while another rule generates $u^{n+1}$, then these two rules
can be used to iteratively generate $u^n$ for $n=0,1, \cdots$.  Thus
by resolving the rules that provide the interface and boundary
gradients $u_x^n$ a complete schedule as shown in table
\ref{table3:schedule} can be derived from the given specification.

\begin{table}[htbp]
\caption{ A Deduced Execution Schedule for the Example
  Diffusion Problem}
\begin{center}
  \begin{tabular}{|l|l|l|l|}
    \hline
    Rule Used & variable computed  & context          & comment \\
    \hline
    Rule 1 & compute $x_c$         & $c=N+1, \cdots, 2N$ & cell centers\\
    Rule 6 & compute $u^{n=0}_c$   & $c=N+1, \cdots, 2N$ & initial conditions \\
    Loop   &                       & define $n=0$     & for $n=0,\cdots$ \\
    Rule 4 & compute $(u_x)^n_i$   & $i=0$       & left boundary condition\\
    Rule 5 & compute $(u_x)^n_i$   & $i=N$       & right boundary condition\\
    Rule 3 & compute $(u_x)^n_i$   & $i=1..N-1$  & diffusion flux at time $n$\\
    Rule 2 & compute $u^{n+1}_i$   & $i=0..N$    & advance time-step\\
    End Loop &                     & loop to Rule 4 & replace $n=n+1$, repeat\\
    \hline
  \end{tabular}
\end{center}
\label{table3:schedule}
\end{table}

\subsection{Variations: Reductions}

The specification of the finite volume solution method for the
one-dimensional diffusion equation has been given, but this simple
example has left out some important infrastructure that will require
special treatment if efficient implementations are desired.  The
choice of a time integration used in this example is not stable for
any time step, $\Delta t$, and in a typical application a stable
time-step would be computed as part of the iteration calculation.  For
non-linear equations the stable time-step is a function of mesh
spacing and dependent variables and as such can not be computed in
advance.  The usual approach is to compute the maximum stable
time-step for each cell in the domain and then choose the smallest of
these time-steps as the global time-step control.  It is important to
notice that the time-step is a single value that is a function of a
set of entities (namely cells).  In addition, this is not any ordinary
function, it is a function obtained by applying an operation to some
set of values.  The result of such a process is always that the set of
values becomes reduced to a single value by a process called
reduction.  Reduction processes are common in the implementation of
unstructured applications.  There are basically two forms of
reductions that are found in these applications.  One, like the
time-step control discussed here, involves applying the reduction over
a large set of entities to obtain a global quantity, while the other
involves reducing through mappings from one set of entities to another
to obtain a set of local quantities (as in summing forces from cells
to nodes).  The second of these operations is often used to reduce the
number of connectivity lists required by the unstructured algorithm,
since reductions through maps can represent functions requiring
inverses of the same map.  Thus using these local reductions it is
possible to save computing and storing an inverse of an existing map.
Since reductions are quite common in unstructured applications, some
support for these cases must be provided by any specification system
for unstructured numerical algorithms.  The particular approach
presented in this thesis will be discussed in the subsequent sections.



\section{Loci Helper Classes}

Loci also provides a few helper classes that are often useful in
numerical computations.  One is the {\tt Array} template class which provides a
mechanism for creating Arrays as first class objects that are
appropriate for using as classes used in templated containers. ({\it
Never use C arrays in templated containers.  Their semantics are
different from other C++ objects and may break templated code in
unexpected ways.})  In addition to the {\tt Array} template class,
classes for three and two dimensional vectors are also provided.  See
the following example code to see how to use these helper classes.

\include{helpers_cc}

\chapter{Representing and Manipulating Data Structures}
\section{Creating Data Structures in Loci}

In Loci, data structures are represented as relations between entities
using Maps.  These data structures, and associated initial values, are
stored in a repository called the fact database which is managed by
the {\tt fact\_db} class.  The fact database is what Loci uses to
coordinate values between computations.  All Loci containers, (e.g.
store, Map, etc.) can be registered into the fact database.

For example, we create a fact database that contains the fact ``value'' using the {\tt create\_fact} member function as follows:
\begin{verbatim}
      // Create a Fact Database
      fact_db facts ;

      // Create a new value
      store<double> value ;
      // ...
      // ... Read In Values
      // ...
   
      // Insert value into the fact database
      facts.create_fact("value",value) ;
\end{verbatim}

Once containers are registered in the fact database, we can later
extract the values that were placed in the fact database using the 
{\tt get\_fact} member function.  For example, we can obtain the
current values from the fact database as follows:
\begin{verbatim}
      store<double> value(facts.get_fact("value")) ;
\end{verbatim}

Also if we wish to allocate new entities, we can ask the fact database
for a new allocation by calling the method {\tt get\_allocation} with
an argument of the number of entities that you need allocated.  For
example:
\begin{verbatim}
      entitySet node_entities = facts.get_allocation(number_of_nodes) ;
\end{verbatim}

For a more detailed example, refer to the subroutine listed below
which reads in a file that defines a triangulated mesh.  The mesh file
consists of a list of points in 2-D space and a list of triangles that
are formed using those points.  This file reader installs the node
positions and the triangle definitions in the fact database passed
into its argument list.  The mesh is defined by two Loci containers.
One is a {\tt store} that contains 2-D vectors named ``pos'' that
defines the nodal positions and a {\tt MapVec} named
``triangle\_nodes'' that defines the three nodes that form a triangle
in a counter-clockwise ordering.

\include{grid_reader_cc}

\section{Transforming Data structures in Loci}

We may need to transform data structures from one form to another
before they are useful for computations.  For example, if we are
to use the mesh of triangles read in the previous section in a
finite-volume based algorithm, we would usually need an edge-centric
data structure rather than a cell-centric data structure that the
triangle definition provides.  So, we would like to convert this
triangle based data structure to one that consists of edges.  Each
edge is defined by two nodes and two cells on either side.  Boundary
edges will be require special treatment since there is no outer
triangle.  We will create a ``ghost'' cell in these cases to make the
data structure consistent for all edges.  Also, we will want to make
sure that each edge is represented in this data structure only once.

We create this edge-based data structure by looping over the three
edges of each triangle, and by searching neighboring triangles to find a
triangle that shares the same edge.  Once we find this triangle, then
we know the two nodes and two cells that form an edge.  In order to
make sure that we don't find the same edge twice, we mark triangles
that we visit and only insert the edge we find when none of the
triangles have been visited before.  If we can't find a matching
triangle, then we know the edge must be a boundary edge that requires
a ghost cell allocation.

In order to search only the neighboring triangles, we need to
transpose the ``triangle\_nodes'' in order to obtain a mapping from
nodes to the triangles that are defined using that node.  A transpose
of a map is found using the {\tt inverseMap} function.  This function has
four arguments.  The first is a {\tt multiMap} that is returned by the
function.  The second is the {\tt Map} that will be transposed.
Following this are the entities for which the transposed map will be
defined (in this case it will be all the nodes in the problem).  The
final argument is the entities that describe the region of the map
that will be transposed.  In this case, we are interested in
transposing the {\tt triangle\_node} map for all triangles.  Therefore
to get the map from nodes to all neighboring triangles we execute the
function 
\begin{verbatim}
  Loci::inverseMap(nodes2tri,triangle_nodes,node_set,triangle_set) ;
\end{verbatim}

Once we have the map from nodes to triangles, we can choose one of the
nodes of an edge to search for the triangle that contains the same
edge.  Once we have set up the preliminaries, we use a vector to store
each edge as we find it.  Once we know how many edges are in the mesh,
we can allocate entities for the edges and create the maps
representing the edge data structure.  For this we create three maps,
``cl'', ``cr'', and ``edge\_nodes'' representing the left and right
cells and the two nodes that define the edge.  The following code
shows how to set up these data structures using Loci.

\include{setup_edges_cc}

\chapter{Computational Rule Specification}

In Loci, computations are performed by executing rules.  At a high
level, it is useful to think of Loci as a ``make'' program for
managing simulation computations instead of compilations.  A Loci rule
includes a documentation section that describes the values that it
depends upon and the values that it produces, combined with a
computation method that can perform the documented computation when
needed.  The following sections will describe how to perform various
types of computations using Loci rules.

\section{Rule Signatures}

Before we begin describing Loci rules, we should first describe how we
name Loci rules.  Rule signatures are the names of Loci rules.  Loci
will only allow one computation of a given rule signature, so we can
use rule signatures to identify any given computational component in a
Loci program.  The simplest rule signature just shows outputs of the
rule followed by the inputs.  The inputs and outputs are divided by
the ``{\tt <- }'' symbol.  For example, a rule that inputs values {\tt
  A} and {\tt B} to produce output {\tt C} has a rule signature as
follows:

\begin{verbatim}
C<-A,B
\end{verbatim}

If relations (such as the {\tt Map} {\tt cl}
and the {\tt Map} {\tt cr} in the edge-based data structure of the
previous section) are used in a computation, then they are included 
in the rule signature.
For example, a computation that computes a value {\tt D }
by averaging value {\tt C } on both sides of any given face would
be documented by the rule signature:

\begin{verbatim}
D<-(cl,cr)->C
\end{verbatim}

What this rule signature means is that {\tt D } is computed by
accessing {\tt C } through the relations identified as 
{\tt cl } and {\tt cr }.  Note that in these rule signatures, 
the comma binds more weakly than
the mapping right arrow operator.  The
parentheses, therefore, are required.

\section{Rule Databases}

Rules are managed through the use of a rule database class called
``{\tt rule\_db }''.  Generally users put rules in the rule database
from the {\tt global\_rule\_list }, a list of rules that is created
before {\tt main} is called by {\tt register\_rule } templates.  As a
result, the usual way that a rule database is manipulated in Loci is
to insert all of the rules in the global rule list.  This is
accomplished with the following code segment: 

\begin{verbatim}
  ////////////////////////////////////////////////////////////////////
  // rule_db and global_rule_list are defined in Loci.h
  ////////////////////////////////////////////////////////////////////
  // Create a rule database called rdb
  rule_db rdb ;

  ////////////////////////////////////////////////////////////////////
  // Add all of the rules that were inserted into the global_rule_list
  // by register_rule<> types into the rule database rdb
  ////////////////////////////////////////////////////////////////////
  rdb.add_rules(global_rule_list) ;
\end{verbatim}

\section{Creating an execution schedule}

Once we have a database of facts and a database of computational
rules, Loci can use these databases to satisfy queries.  We obtain an
execution schedule that can compute these results using the function
{\tt create\_execution\_schedule} provided by Loci.  This function
takes three arguments, 1) the rule database of computations to use for
this query, 2) the fact database of data structures and initial
values, and 3) a C++ string that contains the variable name that we
wish to query for.  This function returns an {\tt executeP} which is a
pointer to the execution schedule.  {\tt executeP} is a counted
pointer that will automatically delete the memory allocated to the
schedule when the object is destructed.  If a schedule could not be
determined, {\tt create\_execution\_schedule} returns a null pointer.  

We can then execute the schedule by calling the execute member
function of the execution schedule, as in
\begin{verbatim}
  schedule->execute(facts) ;
\end{verbatim}

As an additional detail, if we wish to execute on a distributed memory
machine, we must first distribute the fact database across processors
before beginning the scheduling.  This is accomplished using the {\tt
  generate\_distribution} and {\tt distribute\_facts} functions
provided by Loci.  See the below code segment for a more complete
example.

\begin{verbatim}
  ////////////////////////////////////////////////////////////////////
  // Here we distribute the fact database, if we are running on
  // multiple processors.  If we are running serially then these
  // operations have no effect.
  // First we obtain a partition of entities based on the current
  // rules and facts
  ////////////////////////////////////////////////////////////////////
  std::vector<entitySet> partition = Loci::generate_distribution(facts,rdb) ;
  ////////////////////////////////////////////////////////////////////
  // Now we use this distribution to partition the facts to processors.
  // The assumption at this point is that every processor has 
  // identical facts and rdb.
  ////////////////////////////////////////////////////////////////////
  Loci::distribute_facts(partition, facts, rdb) ;

  ////////////////////////////////////////////////////////////////////
  // Here we ask Loci to create an execution schedule that will use
  // the rules in the rule database ``rdb'' and the data in the
  // fact database ``facts'' to obtain the variable(s) specified in
  // query.  Here query is a c++ string that contains the name of
  // the variable that we are querying for (or a comma separated
  // list if we wish to query for more than one variable).
  ////////////////////////////////////////////////////////////////////
  executeP schedule = create_execution_schedule(rdb,facts,query) ;

  ////////////////////////////////////////////////////////////////////
  // If Loci is unable to derive an execution schedule it will return
  // a null pointer.
  if(schedule == 0) {
    // Output a diagnostic if no schedule can be obtained
    cerr << "unable to produce execution schedule to satisfy query for "
         << query << endl ;
  } else {
    //////////////////////////////////////////////////////////////////
    // here we actually execute the computation schedule.  The
    // result will be placed into the fact database ``facts'' for
    // later extraction.
    //////////////////////////////////////////////////////////////////
    schedule->execute(facts) ;

    //////////////////////////////////////////////////////////////////
    // Here the execution is complete and the requested computations
    // are in facts.
  }
\end{verbatim}




\section{Pointwise Computations}

A rule is implemented as a class that provides a constructor that
documents the inputs and outputs to the computation and a virtual
compute method that provides computations for arbitrary collections of
entities.  The most fundamental of these computations is the pointwise
rule.  The pointwise rule represents a computation that can be applied
individually, entity by entity.  For example, consider the case of
computing the area of each triangle in the triangular mesh described
earlier.  In order to compute the area of any given triangle we need
to access the positions of the triangle's three nodes.  These node
positions can be accessed using the {\tt triangle\_nodes} map.  The
areas of all of the triangles in the mesh can be computed by looping
over all entities that have the {\tt triangle\_nodes} defined.  More
precisely, not only must {\tt triangle\_nodes} be defined, but the
entities that this map refers to must also have the attribute {\tt
  pos}.  This set of entities is called the context of the rule and
represents the possible entities over which the computational
subroutine may be called.  A rule that computed areas in such a
fashion would have a rule signature of
\begin{verbatim}
area<-triangle_nodes->pos
\end{verbatim}
where area would be the attribute that this computational routine
would provide when the inputs are provided.  

In order to define a pointwise rule, the user defines a class that
inherits from the class {\tt pointwise\_rule} that is provided in {\tt
  Loci.h}.  This class will contain the {\tt store}, {\tt Map}, and
{\tt param} containers that will be used for the computations.  Input
containers should use the {\tt const\_} prefix which establishes that
the container can only be accessed in a read-only mode.  The
constructor of the rule is responsible for registering the containers
used in the computations and documenting the inputs and outputs of the
rule.  The containers are registered using the rule member function
{\tt name\_store()}.  This function attaches a symbolic name (string)
to the containers so that they can be attached to containers stored in
any given fact database when an execution schedule is formed.
Additionally, {\tt input} and {\tt output} methods are provided to
document both input and output data of the given computation
encapsulated by the rule.  Below, a simple example shows how to create
a pointwise rule to compute areas and centroids of triangles in the
triangular mesh.


\include{cell_props_cc}

\section{Reduction Computations}

There is an alternative approach that we could have taken in the
computation of areas.  Instead of looping over triangles, instead we
could have looped over edges and computed the area contribution
associated with each edge as illustrated in figure \ref{fig:area}.  We
define the edge area contribution as the area of the triangle formed
by the two edge nodes and the centroid of the cell.  Obviously the
triangle area is equal to the sum of all of the edge area
contributions.  This computation method has the advantage that it can
be made to compute the area of any general polygon, whereas the
previous approach was limited only to triangles.  Also, since a
natural way of expressing this computation is to loop over edges
(which is typically what is required in finite-volume computations) we
may be able to dispense with storing the relation {\tt
  triangle\_nodes} all together.  However, if we wish to define this
computation via iteration over edges, we will need to sum up the
results incrementally.  As a result we won't be able to express this
computation as a pointwise rule since the computation of area does not
occur completely point by point, but instead is spread out over
several edge updates.

For this style of computation we need to describe a reduction.  In
Loci we describe a reduction through the use of {\tt unit\_rule} and
{\tt apply\_rule}.  The {\tt unit\_rule} defines the identity of the
operator over which we are doing the reduction, while a set of {\tt
  apply\_rule}s define the summing update that we perform as we are
adding each edges contribution to its neighboring triangles.
Following is an example program that shows how to compute areas using
this edge-centric approach.

\begin{figure}[h]
\centerline{
\epsfxsize=3.2in
\epsfbox{figures/edge_area.eps}}
\caption{Computing Triangle Area by Accumulating Edge Contributions}
\label{fig:area}
\end{figure}

Note that reductions can also be used to compute parameter values.
The specification of the reduction is the same as when the output is
a store, and the resulting parameter is the reduced (accumulated)
value.  In fact, an {\tt apply\_rule} is the only valid rule for
computing parameters from stores.  See the file {\tt stable.cc} in the
finite-volume example for an illustration of how to specify reductions
for computing parameters.

\include{area_reduce_cc}

\section{Singleton Rules and Parameter Computations}

Singleton rules are defined for computations that are performed
exclusively on parameters.  Since these computations are on single
values that are associated with a group of entities, it is not
necessary to perform any loops.  However the attribute that is
computed as a result of the rule is only associated with the entities
that are in the intersection of the entities given the attributes of
all of the inputs.  (Note:  In the parallel implementation, all
parameters are duplicated on each processor, so these singleton rules
are computed on each processor when parallel processing is used [no
communication is invoked].)

\begin{verbatim}
//////////////////////////////////////////////////////////////////////////////
//
//
//  Here we compute a new parameter C by multiplying the values contained in
//  parameters A and B.
//  
//  We define a singleton rule by creating a class that inherits from 
//  the class singleton_rule.
class singleton_example : public singleton_rule {
// Here we define the paramters involved in the computations.
// ** Note, only parameters allowed in singleton computations!
  const_param<double> A ;
  const_param<double> B ;
  param<double> C ;
public:
// Here we provide a constructor that names stores and specifies inputs and
// outputs as before.
  collapse_info() {
    name_store("A",A) ;
    name_store("B",B) ;
    name_store("C",C) ;
    input("A,B") ;
    output("C") ;
  }

  virtual void compute(const sequence &seq) {
// Now in the compute method, we don't loop since we only have a single
// value representing the entire set of entities.  Instead we use the
// dereference operator (*) to access the values.
    *C = (*A)*(*B) ;
  }
} ;

// Now we can register this rule like any other Loci Rule
register_rule<singleton_example> register_singleton_example ;
\end{verbatim}

\section{Iterative Computations}


Iteration is defined by way of three types of rule specifications:
build rules that construct the iteration, advance rules that advance
the iteration, and collapse rules that terminate the iteration.  This
specification follows an analogy to the inductive proof in that build
rules are analogous to an inductive base while advance rules are
analogous to an inductive hypothesis.


Iterations are specified by adding an iteration label to variable
identifiers.  Iteration labels are organized into a hierarchy that is
rooted at stationary time (values that don't iterate).  It is assumed
that computations can proceed at any given iteration level while
accessing values computed at either its iteration level or at parent
levels in the label hierarchy.  The iteration label is just a list of
iterator variables that follows the variable name enclosed in braces.
For example, to access value {\tt v} for iteration {\tt n} we have 
{\tt v\{n\}}.  This relationship between traditional imperative
languages' 
loop nesting and the iteration label hierarchy is shown in figure
\ref{fig:nested}. 

\begin{figure}[h]
\centerline{
\epsfxsize=3.4in
\epsfbox{figures/nested.eps}}
\caption{Nested Loops are characterized by a hierarchy of iteration labels}
\label{fig:nested}
\end{figure}

For example, an iteration where a variable named {\tt q} is iterated
to a converged solution may be described by the following three rules:
1) a build rule of the form {\tt q\{n=0\}<-initial\_condition}, 2) an
advance rule similar to {\tt q\{n+1\}<-q\{n\},delta\_q\{n\}}, and 3)
an iteration collapse rule {\tt
  solution<-q\{n\},CONDITION(converged\{n\})}.  Iteration in this
example proceeds by initializing the first iteration, {\tt q\{n=0\}},
using the build rule.  Next, termination of iteration is checked
by computing {\tt converged}.  If the test succeeds then the collapse
rule terminates the iteration.  Finally the iteration advances in time
by the repeated application of the advance rule which computes values
for {\tt q} for the next iteration ({\tt\{n+1\}}) given current
iteration values at time level {\tt\{n\}}.  Note that the completion
of these rules may require invoking other rules specified in the rule
database.  In this case, rules that compute {\tt converged\{n\}} and
{\tt dq\{n\}} will also need to be scheduled.

To support iteration, variables that exist in lower levels of the
iteration hierarchy are automatically promoted up the iteration
hierarchy.  Thus a variable that is computed in iteration {\tt\{n\}}
is communicated to iteration {\tt\{n,it\}} automatically.  In
addition, rules that are specified completely at the stationary level
will be promoted to any level of the hierarchy.  This allows for the
specification of relations that are iteration independent (for
example, $p = \rho \tilde{R} T$ implies $p^n = \rho^n \tilde{R}^n
T^n$).


\include{integration_cc}

\section{Parametric Rules}

We can also create generic, or parametric rules.  These can be any
type of rule and allows Loci to create new rules by variable
substitution.  The output of a parametric rule includes variable names
that are followed by parenthesis enclosed parameter lists.  When these
parameters occur in the inputs to the rule, they will be substituted
by the provided parameter.  See the gradient routine that follows for
an example of how to develop and use parametric rules.

\include{gradient_cc}

%\include {dbase}
%\include {example}
%\include {make}

%% Appendicies
\appendix

\chapter { Makefile Example}
\label{chap:makefile}
\include{Makefile_ex}


%\appendix{datatype}
\include {datatype}
%\appendix{io}
\include {io}

\include{module}
%\include{third_party}


\end{document}

